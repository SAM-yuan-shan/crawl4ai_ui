"""
Crawl4AI ç»¼åˆåŠŸèƒ½æµ‹è¯•è„šæœ¬
æ•´åˆæ‰€æœ‰åŠŸèƒ½ï¼šåŸºç¡€çˆ¬å–ã€é«˜çº§é…ç½®ã€å†…å®¹è¿‡æ»¤ã€PDFå¯¼å‡ºã€æˆªå›¾ç­‰
"""

import asyncio
import nest_asyncio
import os
from datetime import datetime
import json
import base64
from pathlib import Path

# åº”ç”¨nest_asyncioä»¥æ”¯æŒåœ¨å·²æœ‰äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œ
nest_asyncio.apply()

# Crawl4AIç›¸å…³å¯¼å…¥
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.content_filter_strategy import PruningContentFilter, BM25ContentFilter
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

class Crawl4AITester:
    """Crawl4AIç»¼åˆæµ‹è¯•ç±»"""
    
    def __init__(self, base_output_dir="outputs"):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        self.base_output_dir = Path(base_output_dir)
        self.create_output_dirs()
        
    def create_output_dirs(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„"""
        dirs = [
            self.base_output_dir,
            self.base_output_dir / "markdown",
            self.base_output_dir / "pdf", 
            self.base_output_dir / "screenshots",
            self.base_output_dir / "json",
            self.base_output_dir / "reports"
        ]
        for dir_path in dirs:
            dir_path.mkdir(exist_ok=True)
            
    def log_result(self, test_name, result_info):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(f"[{timestamp}] {test_name}: {result_info}")
        
    async def test_basic_crawling(self, url="https://www.anthropic.com"):
        """æµ‹è¯•åŸºç¡€çˆ¬å–åŠŸèƒ½"""
        print("\n=== 1. åŸºç¡€çˆ¬å–æµ‹è¯• ===")
        
        async with AsyncWebCrawler() as crawler:
            # åŸºç¡€çˆ¬å–
            result = await crawler.arun(url=url)
            
            if result.success:
                content_length = len(result.markdown)
                self.log_result("åŸºç¡€çˆ¬å–", f"æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {content_length} å­—ç¬¦")
                
                # ä¿å­˜ç»“æœ
                output_file = self.base_output_dir / "markdown" / "basic_crawl.md"
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(result.markdown)
                    
                return result
            else:
                self.log_result("åŸºç¡€çˆ¬å–", f"å¤±è´¥: {result.error_message}")
                return None
                
    async def test_browser_config(self, url="https://www.anthropic.com"):
        """æµ‹è¯•æµè§ˆå™¨é…ç½®"""
        print("\n=== 2. æµè§ˆå™¨é…ç½®æµ‹è¯• ===")
        
        # è‡ªå®šä¹‰æµè§ˆå™¨é…ç½®
        browser_config = BrowserConfig(
            browser_type="chromium",
            headless=True,
            viewport_width=1920,
            viewport_height=1080,
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        )
        
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(url=url)
            
            if result.success:
                content_length = len(result.markdown)
                self.log_result("æµè§ˆå™¨é…ç½®", f"æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {content_length} å­—ç¬¦")
                
                # ä¿å­˜ç»“æœ
                output_file = self.base_output_dir / "markdown" / "browser_config.md"
                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(result.markdown)
                    
                return result
            else:
                self.log_result("æµè§ˆå™¨é…ç½®", f"å¤±è´¥: {result.error_message}")
                return None
                
    async def test_content_filtering(self, url="https://www.anthropic.com"):
        """æµ‹è¯•å†…å®¹è¿‡æ»¤åŠŸèƒ½"""
        print("\n=== 3. å†…å®¹è¿‡æ»¤æµ‹è¯• ===")
        
        # æµ‹è¯•ä¸åŒçš„å†…å®¹è¿‡æ»¤ç­–ç•¥
        filters = [
            ("åŸå§‹å†…å®¹", None),
            ("Pruningè¿‡æ»¤", PruningContentFilter()),
            ("BM25è¿‡æ»¤", BM25ContentFilter(user_query="ANTHROPIC API AI"))
        ]
        
        results = {}
        
        async with AsyncWebCrawler() as crawler:
            for filter_name, content_filter in filters:
                print(f"\næµ‹è¯• {filter_name}...")
                
                if content_filter:
                    run_config = CrawlerRunConfig(
                        cache_mode=CacheMode.BYPASS,
                        content_filter=content_filter,
                        markdown_generator=DefaultMarkdownGenerator(
                            options={"ignore_links": True, "ignore_images": True}
                        ) if filter_name == "Pruningè¿‡æ»¤" else None
                    )
                else:
                    run_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)
                
                result = await crawler.arun(url=url, config=run_config)
                
                if result.success:
                    content_length = len(result.markdown)
                    self.log_result(filter_name, f"æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {content_length} å­—ç¬¦")
                    
                    # ä¿å­˜ç»“æœ
                    filename = filter_name.replace(" ", "_").lower()
                    output_file = self.base_output_dir / "markdown" / f"{filename}.md"
                    with open(output_file, 'w', encoding='utf-8') as f:
                        f.write(result.markdown)
                        
                    results[filter_name] = {
                        "length": content_length,
                        "success": True,
                        "file": str(output_file)
                    }
                else:
                    self.log_result(filter_name, f"å¤±è´¥: {result.error_message}")
                    results[filter_name] = {
                        "success": False,
                        "error": result.error_message
                    }
        
        return results
        
    async def test_pdf_export(self, url="https://www.anthropic.com"):
        """æµ‹è¯•PDFå¯¼å‡ºåŠŸèƒ½"""
        print("\n=== 4. PDFå¯¼å‡ºæµ‹è¯• ===")
        
        run_config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS,
            pdf=True
        )
        
        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun(url=url, config=run_config)
            
            if result.success and result.pdf:
                pdf_size = len(result.pdf)
                self.log_result("PDFå¯¼å‡º", f"æˆåŠŸï¼ŒPDFå¤§å°: {pdf_size / 1024:.1f} KB")
                
                # ä¿å­˜PDF
                output_file = self.base_output_dir / "pdf" / "comprehensive_test.pdf"
                with open(output_file, 'wb') as f:
                    f.write(result.pdf)
                    
                return {"success": True, "size": pdf_size, "file": str(output_file)}
            else:
                error_msg = result.error_message if not result.success else "PDFç”Ÿæˆå¤±è´¥"
                self.log_result("PDFå¯¼å‡º", f"å¤±è´¥: {error_msg}")
                return {"success": False, "error": error_msg}
                
    async def test_screenshot(self, url="https://www.anthropic.com"):
        """æµ‹è¯•æˆªå›¾åŠŸèƒ½"""
        print("\n=== 5. æˆªå›¾æµ‹è¯• ===")
        
        run_config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS,
            screenshot=True
        )
        
        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun(url=url, config=run_config)
            
            if result.success and result.screenshot:
                # å¤„ç†base64æˆªå›¾æ•°æ®
                try:
                    screenshot_data = base64.b64decode(result.screenshot)
                    screenshot_size = len(screenshot_data)
                    self.log_result("æˆªå›¾", f"æˆåŠŸï¼Œæˆªå›¾å¤§å°: {screenshot_size / 1024 / 1024:.1f} MB")
                    
                    # ä¿å­˜æˆªå›¾
                    output_file = self.base_output_dir / "screenshots" / "comprehensive_test.png"
                    with open(output_file, 'wb') as f:
                        f.write(screenshot_data)
                        
                    return {"success": True, "size": screenshot_size, "file": str(output_file)}
                except Exception as e:
                    self.log_result("æˆªå›¾", f"å¤„ç†å¤±è´¥: {str(e)}")
                    return {"success": False, "error": str(e)}
            else:
                error_msg = result.error_message if not result.success else "æˆªå›¾ç”Ÿæˆå¤±è´¥"
                self.log_result("æˆªå›¾", f"å¤±è´¥: {error_msg}")
                return {"success": False, "error": error_msg}
                
    async def test_metadata_extraction(self, url="https://www.anthropic.com"):
        """æµ‹è¯•å…ƒæ•°æ®æå–"""
        print("\n=== 6. å…ƒæ•°æ®æå–æµ‹è¯• ===")
        
        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun(url=url)
            
            if result.success:
                metadata = {
                    "url": result.url,
                    "title": getattr(result, 'title', 'N/A'),
                    "content_length": len(result.markdown),
                    "links_count": len(result.links.get('internal', [])) + len(result.links.get('external', [])),
                    "internal_links": len(result.links.get('internal', [])),
                    "external_links": len(result.links.get('external', [])),
                    "images_count": len(result.media.get('images', [])) if result.media else 0
                }
                
                self.log_result("å…ƒæ•°æ®æå–", f"æˆåŠŸï¼Œæ ‡é¢˜: {metadata['title']}")
                
                # ä¿å­˜å…ƒæ•°æ®
                output_file = self.base_output_dir / "json" / "metadata.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)
                    
                return metadata
            else:
                self.log_result("å…ƒæ•°æ®æå–", f"å¤±è´¥: {result.error_message}")
                return None
                
    def generate_report(self, test_results):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n=== 7. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š ===")
        
        report = {
            "test_time": datetime.now().isoformat(),
            "crawl4ai_version": "0.6.3",  # å®é™…ç‰ˆæœ¬
            "results": test_results,
            "summary": {
                "total_tests": len(test_results),
                "successful_tests": sum(1 for r in test_results.values() if isinstance(r, dict) and r.get('success', False)),
                "failed_tests": sum(1 for r in test_results.values() if isinstance(r, dict) and not r.get('success', True))
            }
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.base_output_dir / "reports" / f"test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
            
        # ç”Ÿæˆç®€è¦æŠ¥å‘Š
        summary_file = self.base_output_dir / "reports" / "latest_summary.txt"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(f"Crawl4AI ç»¼åˆæµ‹è¯•æŠ¥å‘Š\n")
            f.write(f"{'='*50}\n")
            f.write(f"æµ‹è¯•æ—¶é—´: {report['test_time']}\n")
            f.write(f"ç‰ˆæœ¬: {report['crawl4ai_version']}\n")
            f.write(f"æ€»æµ‹è¯•æ•°: {report['summary']['total_tests']}\n")
            f.write(f"æˆåŠŸ: {report['summary']['successful_tests']}\n")
            f.write(f"å¤±è´¥: {report['summary']['failed_tests']}\n\n")
            
            for test_name, result in test_results.items():
                if isinstance(result, dict):
                    status = "âœ“" if result.get('success', False) else "âœ—"
                    f.write(f"{status} {test_name}\n")
                else:
                    f.write(f"- {test_name}: {result}\n")
        
        self.log_result("æµ‹è¯•æŠ¥å‘Š", f"å·²ç”Ÿæˆï¼Œä¿å­˜è‡³: {report_file}")
        return report
        
    async def run_comprehensive_test(self, test_url="https://www.anthropic.com"):
        """è¿è¡Œç»¼åˆæµ‹è¯•"""
        print("ğŸš€ å¼€å§‹ Crawl4AI ç»¼åˆåŠŸèƒ½æµ‹è¯•")
        print(f"ç›®æ ‡URL: {test_url}")
        print(f"è¾“å‡ºç›®å½•: {self.base_output_dir}")
        
        test_results = {}
        
        try:
            # 1. åŸºç¡€çˆ¬å–æµ‹è¯•
            basic_result = await self.test_basic_crawling(test_url)
            test_results["åŸºç¡€çˆ¬å–"] = {"success": basic_result is not None}
            
            # 2. æµè§ˆå™¨é…ç½®æµ‹è¯•
            browser_result = await self.test_browser_config(test_url)
            test_results["æµè§ˆå™¨é…ç½®"] = {"success": browser_result is not None}
            
            # 3. å†…å®¹è¿‡æ»¤æµ‹è¯•
            filter_results = await self.test_content_filtering(test_url)
            test_results["å†…å®¹è¿‡æ»¤"] = filter_results
            
            # 4. PDFå¯¼å‡ºæµ‹è¯•
            pdf_result = await self.test_pdf_export(test_url)
            test_results["PDFå¯¼å‡º"] = pdf_result
            
            # 5. æˆªå›¾æµ‹è¯•
            screenshot_result = await self.test_screenshot(test_url)
            test_results["æˆªå›¾"] = screenshot_result
            
            # 6. å…ƒæ•°æ®æå–æµ‹è¯•
            metadata_result = await self.test_metadata_extraction(test_url)
            test_results["å…ƒæ•°æ®æå–"] = {"success": metadata_result is not None, "data": metadata_result}
            
            # 7. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
            report = self.generate_report(test_results)
            
            print("\nğŸ‰ ç»¼åˆæµ‹è¯•å®Œæˆï¼")
            print(f"ğŸ“Š æµ‹è¯•ç»“æœ: {report['summary']['successful_tests']}/{report['summary']['total_tests']} é€šè¿‡")
            
            return test_results
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            return {"error": str(e)}

async def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºæµ‹è¯•å™¨å®ä¾‹
    tester = Crawl4AITester()
    
    # è¿è¡Œç»¼åˆæµ‹è¯•
    results = await tester.run_comprehensive_test()
    
    # å¯ä»¥æµ‹è¯•ä¸åŒçš„ç½‘ç«™
    # results = await tester.run_comprehensive_test("https://example.com")
    
    return results

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    results = asyncio.run(main())
    print("\nâœ… æ‰€æœ‰æµ‹è¯•å·²å®Œæˆ") 