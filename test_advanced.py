#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Crawl4AI é«˜çº§æµ‹è¯•è„šæœ¬
æµ‹è¯•æµè§ˆå™¨é…ç½®ã€å†…å®¹è¿‡æ»¤ã€markdownç”Ÿæˆå™¨ç­‰é«˜çº§åŠŸèƒ½
"""

import asyncio
import os
import nest_asyncio

# å…è®¸åœ¨Jupyterä¸­ä½¿ç”¨å¼‚æ­¥æ“ä½œ
nest_asyncio.apply()

# è®¾ç½®è¾“å‡ºè·¯å¾„
OUTPUT_PATH = 'outputs/markdown/'

def output_md(base_filename, md_str):
    """ä¿å­˜markdownå†…å®¹åˆ°æ–‡ä»¶"""
    os.makedirs(OUTPUT_PATH, exist_ok=True)
    
    length = len(md_str)
    name, ext = os.path.splitext(base_filename)
    filename = f"{name}({length}){ext}"
    
    full_path = os.path.join(OUTPUT_PATH, filename)
    
    with open(full_path, 'w', encoding='utf-8') as f:
        f.write(md_str)
    
    print(f"å·²ä¿å­˜åˆ°: {full_path}")
    return full_path

async def test_browser_config(output_filename):
    """æµ‹è¯•æµè§ˆå™¨é…ç½®åŠŸèƒ½"""
    try:
        print("ğŸ”§ æµ‹è¯•æµè§ˆå™¨é…ç½®...")
        
        from crawl4ai import AsyncWebCrawler, BrowserConfig
        
        # é…ç½®æµè§ˆå™¨å‚æ•°
        browser_config = BrowserConfig(
            headless=True,  # æ— å¤´æ¨¡å¼ï¼Œä¸æ˜¾ç¤ºæµè§ˆå™¨çª—å£
            viewport_width=1280,   # çª—å£å®½åº¦
            viewport_height=720,   # çª—å£é«˜åº¦
            user_agent='Chrome/114.0.0.0',  # æµè§ˆå™¨æ ‡è¯†
            text_mode=True, # ç¦ç”¨å›¾ç‰‡åŠ è½½ï¼Œå¯èƒ½ä¼šåŠ é€Ÿä»…æ–‡æœ¬çš„çˆ¬å–
        )
        
        # åˆ›å»ºå¼‚æ­¥ç½‘é¡µçˆ¬è™«ï¼Œè‡ªåŠ¨ç®¡ç†èµ„æº
        async with AsyncWebCrawler(config=browser_config) as crawler:
            # æ‰§è¡Œç½‘é¡µçˆ¬å–
            result = await crawler.arun(
                url="https://www.anthropic.com/news/agent-capabilities-api",  # ç›®æ ‡ç½‘å€
            )
            
            # æ˜¾ç¤ºçˆ¬å–ç»“æœ
            print(f"âœ… æµè§ˆå™¨é…ç½®æµ‹è¯•æˆåŠŸ! Markdowné•¿åº¦: {len(result.markdown)}")
            print("ğŸ“ å‰300å­—ç¬¦é¢„è§ˆ:")
            print(result.markdown[:300])
            
            output_md(output_filename, result.markdown)
            
        return True
        
    except Exception as e:
        print(f"âŒ æµè§ˆå™¨é…ç½®æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_crawler_run_config(output_filename):
    """æµ‹è¯•çˆ¬è™«è¿è¡Œé…ç½®"""
    try:
        print("âš™ï¸ æµ‹è¯•çˆ¬è™«è¿è¡Œé…ç½®...")
        
        from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
        from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
        
        # é…ç½®æµè§ˆå™¨å‚æ•°
        browser_config = BrowserConfig(
            headless=True,
            viewport_width=1280,
            viewport_height=720,
            user_agent='Chrome/114.0.0.0',
            text_mode=True,
        )
        
        # é…ç½®çˆ¬è™«è¿è¡Œå‚æ•°
        run_config = CrawlerRunConfig(
            cache_mode=CacheMode.DISABLED,  # ç¦ç”¨ç¼“å­˜ï¼Œè·å–æœ€æ–°å†…å®¹
            markdown_generator=DefaultMarkdownGenerator(),
        )
        
        # åˆ›å»ºå¼‚æ­¥ç½‘é¡µçˆ¬è™«ï¼Œè‡ªåŠ¨ç®¡ç†èµ„æº
        async with AsyncWebCrawler(config=browser_config) as crawler:
            # æ‰§è¡Œç½‘é¡µçˆ¬å–
            result = await crawler.arun(
                url="https://www.anthropic.com/news/agent-capabilities-api",
                config=run_config,
            )
            
            print(f"âœ… è¿è¡Œé…ç½®æµ‹è¯•æˆåŠŸ! Markdowné•¿åº¦: {len(result.markdown)}")
            output_md(output_filename, result.markdown)
            
        return True
        
    except Exception as e:
        print(f"âŒ è¿è¡Œé…ç½®æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_content_filter_pruning(output_filename):
    """æµ‹è¯•PruningContentFilterå†…å®¹è¿‡æ»¤"""
    try:
        print("ğŸ” æµ‹è¯•PruningContentFilterå†…å®¹è¿‡æ»¤...")
        
        from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode, BrowserConfig
        from crawl4ai.content_filter_strategy import PruningContentFilter
        from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
        
        # æµè§ˆå™¨é…ç½®
        browser_config = BrowserConfig(
            headless=True,
            viewport_width=1280,
            viewport_height=720,
            user_agent='Chrome/114.0.0.0',
            text_mode=True,
        )
        
        # çˆ¬è™«è¿è¡Œé…ç½®
        run_config = CrawlerRunConfig(
            cache_mode=CacheMode.DISABLED,
            markdown_generator=DefaultMarkdownGenerator(
                content_filter=PruningContentFilter(
                    threshold=0.76,  # è¿‡æ»¤é˜ˆå€¼
                    threshold_type="fixed",  # å›ºå®šé˜ˆå€¼
                ),
            ),
        )
        
        # åˆ›å»ºçˆ¬è™«å¹¶æ‰§è¡Œ
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(
                url="https://www.anthropic.com/news/agent-capabilities-api",
                config=run_config,
            )
            
            # ä¿å­˜åŸå§‹å†…å®¹
            print(f"åŸå§‹Markdowné•¿åº¦: {len(result.markdown.raw_markdown)}")
            output_md(output_filename.replace('.md', '_raw.md'), result.markdown.raw_markdown)
            
            # ä¿å­˜è¿‡æ»¤åå†…å®¹
            print(f"âœ… è¿‡æ»¤åMarkdowné•¿åº¦: {len(result.markdown.fit_markdown)}")
            output_md(output_filename.replace('.md', '_fit.md'), result.markdown.fit_markdown)
            
        return True
        
    except Exception as e:
        print(f"âŒ å†…å®¹è¿‡æ»¤æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_content_filter_with_options(output_filename):
    """æµ‹è¯•å¸¦Optionsçš„å†…å®¹è¿‡æ»¤"""
    try:
        print("ğŸ“ æµ‹è¯•å†…å®¹è¿‡æ»¤ + Options...")
        
        from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode, BrowserConfig
        from crawl4ai.content_filter_strategy import PruningContentFilter
        from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
        
        # æµè§ˆå™¨é…ç½®
        browser_config = BrowserConfig(
            headless=True,
            viewport_width=1280,
            viewport_height=720,
            user_agent='Chrome/114.0.0.0',
            text_mode=True,
        )
        
        # çˆ¬è™«è¿è¡Œé…ç½®
        run_config = CrawlerRunConfig(
            cache_mode=CacheMode.DISABLED,
            markdown_generator=DefaultMarkdownGenerator(
                content_filter=PruningContentFilter(
                    threshold=0.76,
                    threshold_type="dynamic",  # åŠ¨æ€é˜ˆå€¼
                ),
                options={
                    "ignore_links": True,     # ç§»é™¤è¶…é“¾æ¥
                    "ignore_images": True,    # ç§»é™¤å›¾ç‰‡å¼•ç”¨
                }
            ),
        )
        
        # åˆ›å»ºçˆ¬è™«å¹¶æ‰§è¡Œ
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(
                url="https://www.anthropic.com/news/agent-capabilities-api",
                config=run_config,
            )
            
            # ä¿å­˜åŸå§‹å†…å®¹
            print(f"åŸå§‹Markdowné•¿åº¦: {len(result.markdown.raw_markdown)}")
            output_md(output_filename.replace('.md', '_raw.md'), result.markdown.raw_markdown)
            
            # ä¿å­˜è¿‡æ»¤åå†…å®¹
            print(f"âœ… è¿‡æ»¤åMarkdowné•¿åº¦: {len(result.markdown.fit_markdown)}")
            output_md(output_filename.replace('.md', '_fit.md'), result.markdown.fit_markdown)
            
        return True
        
    except Exception as e:
        print(f"âŒ Optionsæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_bm25_content_filter(output_filename):
    """æµ‹è¯•BM25å†…å®¹è¿‡æ»¤"""
    try:
        print("ğŸ”¤ æµ‹è¯•BM25å†…å®¹è¿‡æ»¤...")
        
        from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode, BrowserConfig
        from crawl4ai.content_filter_strategy import BM25ContentFilter
        from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
        
        # æµè§ˆå™¨é…ç½®
        browser_config = BrowserConfig(
            headless=True,
            viewport_width=1280,
            viewport_height=720,
            user_agent='Chrome/114.0.0.0',
            text_mode=True,
        )
        
        # çˆ¬è™«è¿è¡Œé…ç½®
        run_config = CrawlerRunConfig(
            cache_mode=CacheMode.DISABLED,
            markdown_generator=DefaultMarkdownGenerator(
                content_filter=BM25ContentFilter(
                    user_query="ANTHROPIC API",  # å…³é”®è¯æŸ¥è¯¢
                    threshold=1.2,  # BM25é˜ˆå€¼
                ),
                options={
                    "ignore_links": True,
                    "ignore_images": True,
                }
            ),
        )
        
        # åˆ›å»ºçˆ¬è™«å¹¶æ‰§è¡Œ
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(
                url="https://www.anthropic.com/news/agent-capabilities-api",
                config=run_config,
            )
            
            # ä¿å­˜åŸå§‹å†…å®¹
            print(f"åŸå§‹Markdowné•¿åº¦: {len(result.markdown.raw_markdown)}")
            output_md(output_filename.replace('.md', '_raw.md'), result.markdown.raw_markdown)
            
            # ä¿å­˜è¿‡æ»¤åå†…å®¹
            print(f"âœ… BM25è¿‡æ»¤åMarkdowné•¿åº¦: {len(result.markdown.fit_markdown)}")
            output_md(output_filename.replace('.md', '_fit.md'), result.markdown.fit_markdown)
            
        return True
        
    except Exception as e:
        print(f"âŒ BM25è¿‡æ»¤æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹Crawl4AIé«˜çº§åŠŸèƒ½æµ‹è¯•...")
    print("=" * 60)
    
    # 1. æµ‹è¯•æµè§ˆå™¨é…ç½®
    if not await test_browser_config('2_1_BrowserConfig.md'):
        return
        
    print("-" * 60)
    
    # 2. æµ‹è¯•çˆ¬è™«è¿è¡Œé…ç½®
    if not await test_crawler_run_config('2_2_0_RunConfig.md'):
        return
        
    print("-" * 60)
    
    # 3. æµ‹è¯•å†…å®¹è¿‡æ»¤ - Pruning
    if not await test_content_filter_pruning('2_2_1_ContentFilterPruning.md'):
        return
        
    print("-" * 60)
    
    # 4. æµ‹è¯•å†…å®¹è¿‡æ»¤ + Options
    if not await test_content_filter_with_options('2_2_2_ContentFilterPruning_Options.md'):
        return
        
    print("-" * 60)
    
    # 5. æµ‹è¯•BM25å†…å®¹è¿‡æ»¤
    if not await test_bm25_content_filter('2_2_3_BM25ContentFilter.md'):
        return
        
    print("=" * 60)
    print("ğŸ‰ æ‰€æœ‰é«˜çº§åŠŸèƒ½æµ‹è¯•é€šè¿‡!")

if __name__ == "__main__":
    asyncio.run(main()) 