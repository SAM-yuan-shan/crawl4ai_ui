# Crawl4AI UI 使用指南

## 📋 目录
- [快速开始](#快速开始)
- [界面介绍](#界面介绍)
- [功能详解](#功能详解)
- [使用示例](#使用示例)
- [常见问题](#常见问题)
- [故障排除](#故障排除)

## 🚀 快速开始

### 方法一：双击启动（推荐）
1. 双击 `启动界面.bat` 文件
2. 等待依赖检查和安装完成
3. 界面会自动打开

### 方法二：命令行启动
```bash
# 安装依赖
pip install crawl4ai nest-asyncio

# 启动界面
python start_ui.py
```

### 方法三：直接运行
```bash
python crawl4ai_ui.py
```

## 🖥️ 界面介绍

### 主界面布局
```
┌─────────────────────────────────────────────────────────┐
│                    🌐 标题栏                            │
├─────────────────────────────────────────────────────────┤
│ 🎯 目标网址区域                                         │
│   ├── URL输入框                                        │
│   ├── 快速选择按钮                                      │
│   └── 输出目录选择                                      │
├─────────────────┬───────────────────────────────────────┤
│ 🌍 浏览器设置    │ 🔍 内容过滤                          │
│   ├── 浏览器类型 │   ├── 过滤模式                       │
│   ├── 无头模式   │   ├── 关键词输入                     │
│   └── 窗口大小   │   └── 说明文字                       │
├─────────────────┴───────────────────────────────────────┤
│ 📥 导出选项                                             │
│   ├── □ Markdown  □ PDF  □ 截图  □ 信息                │
├─────────────────────────────────────────────────────────┤
│ 📋 批量处理区域                                         │
│   ├── URL列表输入框                                     │
│   └── 文件操作按钮                                      │
├─────────────────────────────────────────────────────────┤
│ 🎮 控制按钮区域                                         │
│   ├── 🚀开始爬取  ⏹️停止  🗂️打开目录  🧹清理  ⚙️重置    │
├─────────────────────────────────────────────────────────┤
│ 📋 运行日志区域                                         │
│   ├── 滚动文本框（显示实时日志）                         │
│   └── 🗑️清空日志  💾保存日志                            │
├─────────────────────────────────────────────────────────┤
│ 📊 状态栏                                               │
│   ├── 状态文字                                          │
│   └── 进度条                                            │
└─────────────────────────────────────────────────────────┘
```

## 🔧 功能详解

### 1. 🎯 目标网址设置

#### URL输入
- **作用**: 输入要爬取的网页地址
- **格式**: 完整的URL，包含 `http://` 或 `https://`
- **示例**: `https://www.anthropic.com`

#### 快速选择
- **Anthropic**: AI公司官网，内容丰富
- **OpenAI**: ChatGPT开发公司
- **百度**: 中文搜索引擎首页
- **GitHub**: 代码托管平台

#### 输出目录
- **默认**: `outputs` 文件夹
- **自定义**: 点击"浏览"按钮选择目录
- **建议**: 选择有足够空间的目录

### 2. 🌍 浏览器设置

#### 浏览器类型
| 类型 | 特点 | 适用场景 |
|------|------|----------|
| **Chromium** | 兼容性最好，推荐选择 | 大部分网站 |
| **Firefox** | 适合某些特殊网站 | 反爬虫较强的网站 |
| **Webkit** | 轻量级，速度较快 | 简单页面 |

#### 无头模式
- **开启** ✅: 后台运行，不显示浏览器窗口（推荐）
- **关闭** ❌: 显示浏览器窗口，可以看到爬取过程

#### 窗口大小
- **宽度**: 影响页面布局，建议1280px
- **高度**: 影响内容加载，建议720px
- **注意**: 某些响应式网站会根据窗口大小显示不同内容

### 3. 🔍 内容过滤

#### 过滤模式对比
| 模式 | 效果 | 压缩率 | 适用场景 |
|------|------|--------|----------|
| **无过滤** | 保留原始内容 | 0% | 需要完整内容 |
| **智能修剪** | 移除导航、广告等 | 30-50% | 提取主要内容 |
| **关键词过滤** | 只保留相关内容 | 60-90% | 特定信息提取 |

#### 关键词过滤说明
- **输入格式**: 用空格分隔多个关键词
- **示例**: `人工智能 AI 机器学习`
- **算法**: 基于BM25相关性算法
- **阈值**: 自动调节，保留最相关的内容

### 4. 📥 导出选项

#### Markdown文档 📄
- **优点**: 体积小，格式清晰，易于编辑
- **用途**: 内容分析、文档整理
- **文件大小**: 通常几KB到几百KB

#### PDF文档 📑
- **优点**: 格式固定，适合保存和分享
- **用途**: 报告生成、存档
- **注意**: 生成时间较长，文件较大
- **文件大小**: 通常几百KB到几MB

#### 网页截图 📸
- **优点**: 保留完整视觉效果
- **用途**: 网页快照、设计参考
- **注意**: 文件很大，建议只在必要时使用
- **文件大小**: 通常几MB到几十MB

#### 页面信息 ℹ️
- **内容**: URL、标题、字数、链接数、图片数等
- **格式**: JSON格式，便于程序处理
- **用途**: 统计分析、批量处理
- **文件大小**: 通常几KB

### 5. 📋 批量处理

#### URL输入格式
```
https://www.example1.com
https://www.example2.com/page1
https://www.example3.com/article/123
```

#### 文件操作
- **从文件加载** 📂: 支持.txt格式的URL列表
- **保存到文件** 💾: 将当前URL列表保存
- **清空** 🗑️: 清除所有输入的URL

#### 批量处理特点
- **顺序处理**: 按输入顺序依次爬取
- **错误继续**: 单个失败不影响其他URL
- **文件命名**: 自动编号，避免冲突
- **进度显示**: 实时显示处理进度

## 📝 使用示例

### 示例1：简单网页爬取
1. **输入URL**: `https://www.anthropic.com`
2. **选择导出**: ✅ Markdown文档
3. **点击开始**: 🚀 开始爬取
4. **查看结果**: 在 `outputs/markdown/` 目录

### 示例2：关键词内容提取
1. **输入URL**: `https://openai.com`
2. **过滤模式**: BM25关键词过滤
3. **输入关键词**: `ChatGPT API 人工智能`
4. **选择导出**: ✅ Markdown + ✅ 页面信息
5. **开始爬取**: 获得高度相关的内容

### 示例3：完整页面存档
1. **输入URL**: `https://github.com/microsoft/playwright`
2. **选择导出**: ✅ 全部选项
3. **浏览器设置**: Chromium + 无头模式
4. **开始爬取**: 获得多格式的完整存档

### 示例4：批量新闻爬取
1. **批量URL输入**:
   ```
   https://news.ycombinator.com
   https://techcrunch.com
   https://arstechnica.com
   ```
2. **过滤模式**: 智能修剪
3. **选择导出**: ✅ Markdown + ✅ 页面信息
4. **开始爬取**: 获得多个新闻网站的内容

## ❓ 常见问题

### Q1: 程序启动失败
**A**: 检查以下问题：
- Python版本是否3.7+
- 是否安装了crawl4ai: `pip install crawl4ai`
- 网络连接是否正常

### Q2: 爬取失败
**A**: 可能的原因：
- 网址无法访问
- 网站有反爬虫机制
- 网络连接不稳定
- 浏览器组件下载失败

### Q3: PDF生成失败
**A**: 解决方法：
- 确保有足够的内存空间
- 尝试更换浏览器类型
- 检查目标网页是否支持打印

### Q4: 截图文件过大
**A**: 优化建议：
- 调整窗口大小到较小值
- 避免批量截图
- 定期清理输出目录

### Q5: 批量处理中断
**A**: 检查要点：
- URL格式是否正确
- 网络连接是否稳定
- 硬盘空间是否充足

### Q6: 内容过滤效果不佳
**A**: 调整建议：
- BM25过滤：调整关键词
- 智能修剪：尝试不同浏览器
- 无过滤：获取原始内容手动处理

## 🔧 故障排除

### 依赖问题
```bash
# 重新安装crawl4ai
pip uninstall crawl4ai
pip install crawl4ai

# 安装额外依赖
pip install nest-asyncio playwright

# 安装浏览器
playwright install
```

### 权限问题
```bash
# Windows: 以管理员身份运行
# Linux/Mac: 使用sudo或调整文件权限
chmod +x start_ui.py
```

### 网络问题
- 检查防火墙设置
- 尝试使用VPN
- 更换DNS服务器

### 内存不足
- 关闭其他程序
- 减少并发数量
- 避免同时生成多种大文件

## 📞 获取帮助

1. **查看日志**: 界面底部的运行日志包含详细错误信息
2. **保存日志**: 点击"保存日志"按钮，发送给技术支持
3. **重置配置**: 点击"重置配置"恢复默认设置
4. **重启程序**: 完全关闭后重新启动

## 🎯 最佳实践

### 提高成功率
1. **先测试单个URL** 再进行批量处理
2. **选择合适的过滤模式** 避免内容冗余
3. **合理选择导出格式** 避免文件过大
4. **定期清理输出目录** 释放存储空间

### 提高效率
1. **使用智能修剪** 减少后续处理工作
2. **批量处理同类网站** 复用配置
3. **保存常用URL列表** 避免重复输入
4. **合理设置窗口大小** 平衡效果和速度

### 避免问题
1. **检查网址有效性** 避免死链接
2. **注意目标网站的访问限制** 避免IP被封
3. **监控硬盘空间** 避免写入失败
4. **定期更新依赖** 保持功能稳定

---

**版本**: 1.0.0  
**更新时间**: 2024年12月  
**支持平台**: Windows, macOS, Linux 