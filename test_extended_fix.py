#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä¿®å¤ç‰ˆCrawl4AIæ‰©å±•åŠŸèƒ½æµ‹è¯•è„šæœ¬
"""

import asyncio
import os
import base64
import nest_asyncio

nest_asyncio.apply()

OUTPUT_PATH = 'outputs/'

def create_directories():
    """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
    dirs = ['outputs/markdown', 'outputs/pdf', 'outputs/screenshots']
    for dir_path in dirs:
        os.makedirs(dir_path, exist_ok=True)

async def test_pdf_export():
    """æµ‹è¯•PDFå¯¼å‡ºåŠŸèƒ½"""
    try:
        print("ğŸ“„ æµ‹è¯•PDFå¯¼å‡ºåŠŸèƒ½...")
        
        from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig
        
        # æµè§ˆå™¨é…ç½®
        browser_config = BrowserConfig(
            headless=True,
            viewport_width=1280,
            viewport_height=720,
            user_agent='Chrome/114.0.0.0',
        )
        
        # çˆ¬è™«è¿è¡Œé…ç½®ï¼Œåªå¯ç”¨PDF
        run_config = CrawlerRunConfig(
            pdf=True,      # ç”ŸæˆPDF
        )
        
        # åˆ›å»ºçˆ¬è™«å¹¶æ‰§è¡Œ
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(
                url="https://www.anthropic.com/news/agent-capabilities-api",
                config=run_config,
            )
            
            # ä¿å­˜PDF
            if result.pdf:
                pdf_path = os.path.join(OUTPUT_PATH, 'pdf', 'test_page.pdf')
                with open(pdf_path, 'wb') as f:
                    f.write(result.pdf)
                print(f"âœ… PDFå·²ä¿å­˜åˆ°: {pdf_path} ({len(result.pdf)} å­—èŠ‚)")
            else:
                print("âš ï¸ æœªç”ŸæˆPDF")
            
        return True
        
    except Exception as e:
        print(f"âŒ PDFå¯¼å‡ºæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_screenshot():
    """æµ‹è¯•æˆªå›¾åŠŸèƒ½"""
    try:
        print("ğŸ“· æµ‹è¯•æˆªå›¾åŠŸèƒ½...")
        
        from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig
        
        # æµè§ˆå™¨é…ç½®
        browser_config = BrowserConfig(
            headless=True,
            viewport_width=1280,
            viewport_height=720,
            user_agent='Chrome/114.0.0.0',
        )
        
        # çˆ¬è™«è¿è¡Œé…ç½®ï¼Œåªå¯ç”¨æˆªå›¾
        run_config = CrawlerRunConfig(
            screenshot=True,  # ç”Ÿæˆæˆªå›¾
        )
        
        # åˆ›å»ºçˆ¬è™«å¹¶æ‰§è¡Œ
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(
                url="https://www.anthropic.com/news/agent-capabilities-api",
                config=run_config,
            )
            
            # ä¿å­˜æˆªå›¾
            if result.screenshot:
                screenshot_path = os.path.join(OUTPUT_PATH, 'screenshots', 'test_page.png')
                
                # æ£€æŸ¥æˆªå›¾æ•°æ®ç±»å‹å¹¶é€‚å½“å¤„ç†
                if isinstance(result.screenshot, str):
                    # å¦‚æœæ˜¯base64å­—ç¬¦ä¸²ï¼Œéœ€è¦è§£ç 
                    try:
                        screenshot_data = base64.b64decode(result.screenshot)
                        print("æˆªå›¾æ•°æ®ä¸ºbase64æ ¼å¼ï¼Œå·²è§£ç ")
                    except:
                        # å¦‚æœä¸æ˜¯base64ï¼Œå¯èƒ½æ˜¯å…¶ä»–å­—ç¬¦ä¸²æ ¼å¼
                        screenshot_data = result.screenshot.encode('utf-8')
                        print("æˆªå›¾æ•°æ®ä¸ºå­—ç¬¦ä¸²æ ¼å¼ï¼Œå·²ç¼–ç ")
                else:
                    # å¦‚æœå·²ç»æ˜¯å­—èŠ‚æ•°æ®
                    screenshot_data = result.screenshot
                    print("æˆªå›¾æ•°æ®ä¸ºå­—èŠ‚æ ¼å¼")
                
                with open(screenshot_path, 'wb') as f:
                    f.write(screenshot_data)
                print(f"âœ… æˆªå›¾å·²ä¿å­˜åˆ°: {screenshot_path} ({len(screenshot_data)} å­—èŠ‚)")
            else:
                print("âš ï¸ æœªç”Ÿæˆæˆªå›¾")
            
        return True
        
    except Exception as e:
        print(f"âŒ æˆªå›¾æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_basic_info():
    """æµ‹è¯•åŸºæœ¬ä¿¡æ¯æå–"""
    try:
        print("â„¹ï¸ æµ‹è¯•åŸºæœ¬ä¿¡æ¯æå–...")
        
        from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig
        
        # æµè§ˆå™¨é…ç½®
        browser_config = BrowserConfig(
            headless=True,
            viewport_width=1280,
            viewport_height=720,
            user_agent='Chrome/114.0.0.0',
        )
        
        # åˆ›å»ºçˆ¬è™«å¹¶æ‰§è¡Œ
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(
                url="https://www.anthropic.com/news/agent-capabilities-api",
            )
            
            print("ğŸ“Š é¡µé¢åŸºæœ¬ä¿¡æ¯:")
            print(f"  - URL: {result.url}")
            print(f"  - çŠ¶æ€ç : {result.status_code}")
            print(f"  - Markdowné•¿åº¦: {len(result.markdown)}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é“¾æ¥ä¿¡æ¯
            if hasattr(result, 'links') and result.links:
                internal_count = len(result.links.get('internal', []))
                external_count = len(result.links.get('external', []))
                print(f"  - å†…éƒ¨é“¾æ¥: {internal_count}")
                print(f"  - å¤–éƒ¨é“¾æ¥: {external_count}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰åª’ä½“ä¿¡æ¯
            if hasattr(result, 'media') and result.media:
                print("  - åª’ä½“ä¿¡æ¯:")
                for media_type, items in result.media.items():
                    if isinstance(items, list):
                        print(f"    - {media_type}: {len(items)}ä¸ª")
            
            # ä¿å­˜åŸºæœ¬ä¿¡æ¯
            info_text = f"""é¡µé¢ä¿¡æ¯æŠ¥å‘Š
=============

URL: {result.url}
çŠ¶æ€ç : {result.status_code}
Markdowné•¿åº¦: {len(result.markdown)}

å†…å®¹é¢„è§ˆ (å‰500å­—ç¬¦):
{result.markdown[:500]}
"""
            
            info_path = os.path.join(OUTPUT_PATH, 'page_info.txt')
            with open(info_path, 'w', encoding='utf-8') as f:
                f.write(info_text)
            print(f"ğŸ’¾ é¡µé¢ä¿¡æ¯å·²ä¿å­˜åˆ°: {info_path}")
            
        return True
        
    except Exception as e:
        print(f"âŒ åŸºæœ¬ä¿¡æ¯æå–æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹Crawl4AIæ‰©å±•åŠŸèƒ½æµ‹è¯•...")
    print("=" * 60)
    
    # åˆ›å»ºç›®å½•
    create_directories()
    
    # 1. æµ‹è¯•PDFå¯¼å‡º
    if not await test_pdf_export():
        return
        
    print("-" * 60)
    
    # 2. æµ‹è¯•æˆªå›¾
    if not await test_screenshot():
        return
        
    print("-" * 60)
    
    # 3. æµ‹è¯•åŸºæœ¬ä¿¡æ¯æå–
    if not await test_basic_info():
        return
        
    print("=" * 60)
    print("ğŸ‰ æ‰©å±•åŠŸèƒ½æµ‹è¯•å®Œæˆ!")
    
    # æ˜¾ç¤ºç”Ÿæˆçš„æ–‡ä»¶
    print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    for root, dirs, files in os.walk(OUTPUT_PATH):
        for file in files:
            file_path = os.path.join(root, file)
            file_size = os.path.getsize(file_path)
            print(f"  - {file_path} ({file_size} å­—èŠ‚)")

if __name__ == "__main__":
    asyncio.run(main()) 