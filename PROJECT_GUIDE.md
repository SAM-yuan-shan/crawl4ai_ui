# Crawl4AI å®Œæ•´åŠŸèƒ½æµ‹è¯•é¡¹ç›®

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ Crawl4AI åŠŸèƒ½æµ‹è¯•é¡¹ç›®ï¼ŒæŒ‰ç…§å®˜æ–¹æ•™ç¨‹å’Œæœ€ä½³å®è·µå®ç°äº†æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š

- âœ… åŸºç¡€ç½‘é¡µçˆ¬å–
- âœ… é«˜çº§æµè§ˆå™¨é…ç½®
- âœ… æ™ºèƒ½å†…å®¹è¿‡æ»¤
- âœ… PDFå¯¼å‡ºåŠŸèƒ½
- âœ… ç½‘é¡µæˆªå›¾åŠŸèƒ½
- âœ… å…ƒæ•°æ®æå–
- âœ… æ‰¹é‡æµ‹è¯•å’ŒæŠ¥å‘Šç”Ÿæˆ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚
- Python 3.8+
- ç½‘ç»œè¿æ¥

### å®‰è£…æ­¥éª¤

```bash
# 1. å®‰è£… Crawl4AI
pip install -U crawl4ai

# 2. å®‰è£…å¼‚æ­¥æ”¯æŒ
pip install nest_asyncio

# 3. å®‰è£…æµè§ˆå™¨é©±åŠ¨
python -m playwright install
```

### è¿è¡Œæµ‹è¯•

```bash
# åŸºç¡€åŠŸèƒ½æµ‹è¯•
python test_basic.py

# é«˜çº§åŠŸèƒ½æµ‹è¯•
python test_advanced.py

# æ‰©å±•åŠŸèƒ½æµ‹è¯•
python test_extended_fix.py

# ç»¼åˆåŠŸèƒ½æµ‹è¯•
python comprehensive_test.py
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
b_crawlforai/
â”œâ”€â”€ README.md                 # é¡¹ç›®ç®€ä»‹
â”œâ”€â”€ PROJECT_GUIDE.md         # è¯¦ç»†ä½¿ç”¨æŒ‡å—ï¼ˆæœ¬æ–‡ä»¶ï¼‰
â”œâ”€â”€ test_basic.py            # åŸºç¡€åŠŸèƒ½æµ‹è¯•
â”œâ”€â”€ test_advanced.py         # é«˜çº§é…ç½®æµ‹è¯•
â”œâ”€â”€ test_extended_fix.py     # æ‰©å±•åŠŸèƒ½æµ‹è¯•
â”œâ”€â”€ comprehensive_test.py    # ç»¼åˆæµ‹è¯•è„šæœ¬
â””â”€â”€ outputs/                 # è¾“å‡ºç›®å½•
    â”œâ”€â”€ markdown/           # Markdown æ–‡ä»¶
    â”œâ”€â”€ pdf/               # PDF æ–‡ä»¶
    â”œâ”€â”€ screenshots/       # æˆªå›¾æ–‡ä»¶
    â”œâ”€â”€ json/             # JSON æ•°æ®
    â””â”€â”€ reports/          # æµ‹è¯•æŠ¥å‘Š
```

## ğŸ”§ æ ¸å¿ƒåŠŸèƒ½è¯¦è§£

### 1. åŸºç¡€çˆ¬å–åŠŸèƒ½

**æ–‡ä»¶**: `test_basic.py`

```python
from crawl4ai import AsyncWebCrawler

async with AsyncWebCrawler() as crawler:
    result = await crawler.arun(url="https://example.com")
    print(result.markdown)
```

**ç‰¹ç‚¹**:
- ç®€å•æ˜“ç”¨çš„API
- è‡ªåŠ¨å¤„ç†JavaScriptæ¸²æŸ“
- è¿”å›æ¸…æ™°çš„Markdownæ ¼å¼

### 2. æµè§ˆå™¨é…ç½®

**æ–‡ä»¶**: `test_advanced.py`

```python
from crawl4ai import BrowserConfig

browser_config = BrowserConfig(
    browser_type="chromium",
    headless=True,
    viewport_width=1920,
    viewport_height=1080,
    user_agent="è‡ªå®šä¹‰User-Agent"
)
```

**é…ç½®é€‰é¡¹**:
- æµè§ˆå™¨ç±»å‹ (chromium, firefox, webkit)
- æ— å¤´æ¨¡å¼æ§åˆ¶
- è§†çª—å¤§å°è®¾ç½®
- ç”¨æˆ·ä»£ç†è‡ªå®šä¹‰

### 3. å†…å®¹è¿‡æ»¤ç­–ç•¥

#### PruningContentFilter - æ™ºèƒ½å†…å®¹ä¿®å‰ª

```python
from crawl4ai.content_filter_strategy import PruningContentFilter

content_filter = PruningContentFilter()
# è‡ªåŠ¨ç§»é™¤å¯¼èˆªã€é¡µè„šã€ä¾§è¾¹æ ç­‰å™ªéŸ³å†…å®¹
```

**æ•ˆæœ**: å°†å†…å®¹ä» 10,881 å­—ç¬¦å‹ç¼©åˆ° 7,122 å­—ç¬¦ (65.5%)

#### BM25ContentFilter - å…³é”®è¯è¿‡æ»¤

```python
from crawl4ai.content_filter_strategy import BM25ContentFilter

content_filter = BM25ContentFilter(user_query="ANTHROPIC API")
# åŸºäºå…³é”®è¯ç›¸å…³æ€§è¿‡æ»¤å†…å®¹
```

**æ•ˆæœ**: å°†å†…å®¹ä» 10,881 å­—ç¬¦å‹ç¼©åˆ° 1,860 å­—ç¬¦ (17.1%)

### 4. Markdownç”Ÿæˆé…ç½®

```python
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

markdown_generator = DefaultMarkdownGenerator(
    options={
        "ignore_links": True,    # å¿½ç•¥é“¾æ¥
        "ignore_images": True,   # å¿½ç•¥å›¾ç‰‡
        "body_width": 120       # å†…å®¹å®½åº¦
    }
)
```

### 5. PDFå¯¼å‡ºåŠŸèƒ½

**æ–‡ä»¶**: `test_extended_fix.py`

```python
run_config = CrawlerRunConfig(
    cache_mode=CacheMode.BYPASS,
    pdf=True
)

result = await crawler.arun(url=url, config=run_config)
# result.pdf åŒ…å«PDFäºŒè¿›åˆ¶æ•°æ®
```

**ç‰¹ç‚¹**:
- é«˜è´¨é‡PDFè¾“å‡º
- ä¿æŒåŸå§‹é¡µé¢å¸ƒå±€
- æ”¯æŒå¤§é¡µé¢å®Œæ•´å¯¼å‡º

### 6. ç½‘é¡µæˆªå›¾åŠŸèƒ½

```python
run_config = CrawlerRunConfig(
    cache_mode=CacheMode.BYPASS,
    screenshot=True
)

result = await crawler.arun(url=url, config=run_config)
# result.screenshot åŒ…å«base64ç¼–ç çš„PNGå›¾ç‰‡
```

**ç‰¹ç‚¹**:
- å…¨é¡µé¢æˆªå›¾
- é«˜åˆ†è¾¨ç‡è¾“å‡º
- Base64ç¼–ç ä¾¿äºå¤„ç†

## ğŸ“Š æµ‹è¯•ç»“æœå¯¹æ¯”

| è¿‡æ»¤æ–¹å¼ | å†…å®¹é•¿åº¦ | å‹ç¼©ç‡ | é€‚ç”¨åœºæ™¯ |
|---------|---------|--------|----------|
| åŸå§‹å†…å®¹ | 10,881å­—ç¬¦ | 0% | å®Œæ•´å†…å®¹åˆ†æ |
| PruningContentFilter | 7,122å­—ç¬¦ | 34.5% | ç§»é™¤å™ªéŸ³å†…å®¹ |
| Pruning + Options | 5,987å­—ç¬¦ | 45.0% | è¿›ä¸€æ­¥ç²¾ç®€ |
| BM25å…³é”®è¯è¿‡æ»¤ | 1,860å­—ç¬¦ | 82.9% | ç‰¹å®šä¸»é¢˜æå– |

## ğŸ¯ å®é™…åº”ç”¨åœºæ™¯

### 1. å†…å®¹èšåˆ
- æ–°é—»ç½‘ç«™å†…å®¹æå–
- åšå®¢æ–‡ç« æ”¶é›†
- äº§å“ä¿¡æ¯çˆ¬å–

### 2. æ•°æ®åˆ†æ
- ç«å“åˆ†æ
- å¸‚åœºç ”ç©¶
- ä»·æ ¼ç›‘æ§

### 3. æ–‡æ¡£ç”Ÿæˆ
- ç½‘é¡µè½¬PDF
- å†…å®¹å½’æ¡£
- æŠ¥å‘Šç”Ÿæˆ

### 4. AIè®­ç»ƒæ•°æ®
- æ–‡æœ¬é¢„å¤„ç†
- æ•°æ®æ¸…æ´—
- æ ¼å¼æ ‡å‡†åŒ–

## âš™ï¸ é«˜çº§é…ç½®

### ç¼“å­˜æ§åˆ¶

```python
from crawl4ai import CacheMode

# å¼ºåˆ¶é‡æ–°çˆ¬å–
config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)

# ä½¿ç”¨ç¼“å­˜
config = CrawlerRunConfig(cache_mode=CacheMode.ENABLED)

# ä»…å†™å…¥ç¼“å­˜
config = CrawlerRunConfig(cache_mode=CacheMode.WRITE_ONLY)
```

### ç­‰å¾…ç­–ç•¥

```python
run_config = CrawlerRunConfig(
    wait_for="css:.content",           # ç­‰å¾…CSSé€‰æ‹©å™¨
    delay_before_return_html=2.0,      # å»¶è¿Ÿæ—¶é—´
    page_timeout=30000,                # é¡µé¢è¶…æ—¶
    magic=True                         # æ™ºèƒ½ç­‰å¾…
)
```

### è‡ªå®šä¹‰é’©å­

```python
async def before_goto_hook(page):
    # é¡µé¢å¯¼èˆªå‰æ‰§è¡Œ
    await page.set_extra_http_headers({"Custom-Header": "Value"})

run_config = CrawlerRunConfig(
    hooks={"before_goto": before_goto_hook}
)
```

## ğŸ” é”™è¯¯å¤„ç†

### å¸¸è§é—®é¢˜è§£å†³

1. **ä¾èµ–å†²çªè­¦å‘Š**
   ```
   WARNING: pip's dependency resolver does not currently consider 
   all the packages that are installed
   ```
   - ä¸å½±å“æ ¸å¿ƒåŠŸèƒ½
   - å¯é€šè¿‡è™šæ‹Ÿç¯å¢ƒéš”ç¦»

2. **æµè§ˆå™¨å¯åŠ¨å¤±è´¥**
   ```bash
   # é‡æ–°å®‰è£…æµè§ˆå™¨
   python -m playwright install --force
   ```

3. **å†…å­˜ä¸è¶³**
   ```python
   # å‡å°‘å¹¶å‘æ•°
   browser_config = BrowserConfig(
       browser_type="chromium",
       headless=True,
       chrome_args=["--max_old_space_size=4096"]
   )
   ```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### 1. æ‰¹é‡å¤„ç†

```python
urls = ["url1", "url2", "url3"]
async with AsyncWebCrawler() as crawler:
    tasks = [crawler.arun(url=url) for url in urls]
    results = await asyncio.gather(*tasks)
```

### 2. é€‰æ‹©æ€§æå–

```python
# ä»…æå–æ–‡æœ¬ï¼Œè·³è¿‡å›¾ç‰‡å’Œé“¾æ¥
markdown_generator = DefaultMarkdownGenerator(
    options={"ignore_links": True, "ignore_images": True}
)
```

### 3. æ™ºèƒ½ç¼“å­˜

```python
# é•¿æœŸç¼“å­˜é™æ€å†…å®¹
run_config = CrawlerRunConfig(
    cache_mode=CacheMode.ENABLED,
    bypass_cache=False
)
```

## ğŸŒŸ æœ€ä½³å®è·µ

1. **é€‰æ‹©åˆé€‚çš„è¿‡æ»¤ç­–ç•¥**
   - æ•°æ®åˆ†æ: PruningContentFilter
   - ç‰¹å®šä¿¡æ¯: BM25ContentFilter
   - å®Œæ•´å†…å®¹: æ— è¿‡æ»¤

2. **åˆç†è®¾ç½®æµè§ˆå™¨å‚æ•°**
   - ç”Ÿäº§ç¯å¢ƒ: headless=True
   - è°ƒè¯•ç¯å¢ƒ: headless=False

3. **å¤„ç†å¤§è§„æ¨¡æ•°æ®**
   - ä½¿ç”¨å¼‚æ­¥å¹¶å‘
   - è®¾ç½®åˆç†çš„è¶…æ—¶æ—¶é—´
   - å®ç°é‡è¯•æœºåˆ¶

4. **èµ„æºç®¡ç†**
   - åŠæ—¶å…³é—­crawler
   - æ¸…ç†ä¸´æ—¶æ–‡ä»¶
   - ç›‘æ§å†…å­˜ä½¿ç”¨

## ğŸ“š å‚è€ƒèµ„æº

- [Crawl4AI GitHub](https://github.com/unclecode/crawl4ai)
- [Google Colabæ•™ç¨‹](https://colab.research.google.com/drive/1wz8u30rvbq6Scodye9AGCw8Qg_Z8QGsk)
- [å®˜æ–¹æ–‡æ¡£](https://crawl4ai.com/mkdocs/)
- [Bç«™æ•™ç¨‹è§†é¢‘](https://www.bilibili.com/video/BV1Vx4y1g7bp/)

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ¥æ”¹è¿›è¿™ä¸ªé¡¹ç›®ï¼

## ğŸ“„ è®¸å¯è¯

MIT License - è¯¦è§ LICENSE æ–‡ä»¶

---

**å¼€å‘è€…**: åŸºäºCrawl4AIå®˜æ–¹æ•™ç¨‹å®ç°  
**ç‰ˆæœ¬**: 1.0.0  
**æœ€åæ›´æ–°**: 2024å¹´12æœˆ 