#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Crawl4AI æ‰©å±•åŠŸèƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯•PDFå¯¼å‡ºã€æˆªå›¾ã€é“¾æ¥æå–ã€å›¾ç‰‡ä¿¡æ¯æå–ç­‰æ‰©å±•åŠŸèƒ½
"""

import asyncio
import os
import nest_asyncio

nest_asyncio.apply()

OUTPUT_PATH = 'outputs/'

def create_directories():
    """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
    dirs = ['outputs/markdown', 'outputs/pdf', 'outputs/screenshots']
    for dir_path in dirs:
        os.makedirs(dir_path, exist_ok=True)

async def test_pdf_and_screenshot():
    """æµ‹è¯•PDFå¯¼å‡ºå’Œæˆªå›¾åŠŸèƒ½"""
    try:
        print("ğŸ“„ æµ‹è¯•PDFå¯¼å‡ºå’Œæˆªå›¾åŠŸèƒ½...")
        
        from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig
        
        # æµè§ˆå™¨é…ç½®
        browser_config = BrowserConfig(
            headless=True,
            viewport_width=1280,
            viewport_height=720,
            user_agent='Chrome/114.0.0.0',
        )
        
        # çˆ¬è™«è¿è¡Œé…ç½®ï¼Œå¯ç”¨PDFå’Œæˆªå›¾
        run_config = CrawlerRunConfig(
            pdf=True,      # ç”ŸæˆPDF
            screenshot=True,  # ç”Ÿæˆæˆªå›¾
        )
        
        # åˆ›å»ºçˆ¬è™«å¹¶æ‰§è¡Œ
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(
                url="https://www.anthropic.com/news/agent-capabilities-api",
                config=run_config,
            )
            
            # ä¿å­˜PDF
            if result.pdf:
                pdf_path = os.path.join(OUTPUT_PATH, 'pdf', 'test_page.pdf')
                with open(pdf_path, 'wb') as f:
                    f.write(result.pdf)
                print(f"âœ… PDFå·²ä¿å­˜åˆ°: {pdf_path}")
            
            # ä¿å­˜æˆªå›¾
            if result.screenshot:
                screenshot_path = os.path.join(OUTPUT_PATH, 'screenshots', 'test_page.png')
                with open(screenshot_path, 'wb') as f:
                    f.write(result.screenshot)
                print(f"âœ… æˆªå›¾å·²ä¿å­˜åˆ°: {screenshot_path}")
            
        return True
        
    except Exception as e:
        print(f"âŒ PDFå’Œæˆªå›¾æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_links_extraction():
    """æµ‹è¯•é“¾æ¥æå–åŠŸèƒ½"""
    try:
        print("ğŸ”— æµ‹è¯•é“¾æ¥æå–åŠŸèƒ½...")
        
        from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig
        
        # æµè§ˆå™¨é…ç½®
        browser_config = BrowserConfig(
            headless=True,
            viewport_width=1280,
            viewport_height=720,
            user_agent='Chrome/114.0.0.0',
        )
        
        # çˆ¬è™«è¿è¡Œé…ç½®
        run_config = CrawlerRunConfig(
            # è¿™äº›å‚æ•°ä¸ä¼šå½±å“é“¾æ¥æ”¶é›†ï¼Œé“¾æ¥ä¼šè‡ªåŠ¨å­˜å‚¨åˆ°result.links
            exclude_external_links=False,  # ä¸æ’é™¤å¤–éƒ¨é“¾æ¥
            exclude_social_media_links=False,  # ä¸æ’é™¤ç¤¾äº¤åª’ä½“é“¾æ¥
        )
        
        # åˆ›å»ºçˆ¬è™«å¹¶æ‰§è¡Œ
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(
                url="https://www.anthropic.com/news/agent-capabilities-api",
                config=run_config,
            )
            
            # åˆ†æé“¾æ¥
            if hasattr(result, 'links') and result.links:
                internal_links = result.links.get('internal', [])
                external_links = result.links.get('external', [])
                
                print(f"âœ… å†…éƒ¨é“¾æ¥æ•°é‡: {len(internal_links)}")
                print(f"âœ… å¤–éƒ¨é“¾æ¥æ•°é‡: {len(external_links)}")
                
                # æ˜¾ç¤ºå‰å‡ ä¸ªå†…éƒ¨é“¾æ¥ç¤ºä¾‹
                print("ğŸ“ å†…éƒ¨é“¾æ¥ç¤ºä¾‹ (å‰5ä¸ª):")
                for i, link in enumerate(internal_links[:5]):
                    print(f"  {i+1}. æ–‡å­—: '{link.get('text', '')[:50]}' -> URL: {link.get('href', '')}")
                
                # æ˜¾ç¤ºå‰å‡ ä¸ªå¤–éƒ¨é“¾æ¥ç¤ºä¾‹
                if external_links:
                    print("ğŸŒ å¤–éƒ¨é“¾æ¥ç¤ºä¾‹ (å‰5ä¸ª):")
                    for i, link in enumerate(external_links[:5]):
                        print(f"  {i+1}. æ–‡å­—: '{link.get('text', '')[:50]}' -> URL: {link.get('href', '')}")
                
                # ä¿å­˜é“¾æ¥ä¿¡æ¯åˆ°æ–‡ä»¶
                links_info = f"å†…éƒ¨é“¾æ¥æ•°é‡: {len(internal_links)}\nå¤–éƒ¨é“¾æ¥æ•°é‡: {len(external_links)}\n\n"
                links_info += "=== å†…éƒ¨é“¾æ¥ ===\n"
                for link in internal_links:
                    links_info += f"æ–‡å­—: {link.get('text', '')}\nURL: {link.get('href', '')}\n\n"
                
                if external_links:
                    links_info += "\n=== å¤–éƒ¨é“¾æ¥ ===\n"
                    for link in external_links:
                        links_info += f"æ–‡å­—: {link.get('text', '')}\nURL: {link.get('href', '')}\n\n"
                
                links_path = os.path.join(OUTPUT_PATH, 'links_info.txt')
                with open(links_path, 'w', encoding='utf-8') as f:
                    f.write(links_info)
                print(f"ğŸ’¾ é“¾æ¥ä¿¡æ¯å·²ä¿å­˜åˆ°: {links_path}")
                
            else:
                print("âš ï¸ æœªæ‰¾åˆ°é“¾æ¥ä¿¡æ¯")
                
        return True
        
    except Exception as e:
        print(f"âŒ é“¾æ¥æå–æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_images_extraction():
    """æµ‹è¯•å›¾ç‰‡ä¿¡æ¯æå–åŠŸèƒ½"""
    try:
        print("ğŸ–¼ï¸ æµ‹è¯•å›¾ç‰‡ä¿¡æ¯æå–åŠŸèƒ½...")
        
        from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig
        
        # æµè§ˆå™¨é…ç½®
        browser_config = BrowserConfig(
            headless=True,
            viewport_width=1280,
            viewport_height=720,
            user_agent='Chrome/114.0.0.0',
        )
        
        # çˆ¬è™«è¿è¡Œé…ç½®
        run_config = CrawlerRunConfig()
        
        # åˆ›å»ºçˆ¬è™«å¹¶æ‰§è¡Œ
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(
                url="https://www.anthropic.com/news/agent-capabilities-api",
                config=run_config,
            )
            
            # åˆ†æå›¾ç‰‡ä¿¡æ¯
            if hasattr(result, 'media') and result.media and 'images' in result.media:
                images = result.media['images']
                print(f"âœ… æ‰¾åˆ°å›¾ç‰‡æ•°é‡: {len(images)}")
                
                # æ˜¾ç¤ºå›¾ç‰‡ä¿¡æ¯
                print("ğŸ–¼ï¸ å›¾ç‰‡ä¿¡æ¯:")
                for i, img in enumerate(images[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
                    print(f"  {i+1}. URL: {img.get('src', 'N/A')}")
                    print(f"     Alt: {img.get('alt', 'N/A')}")
                    print(f"     æè¿°: {img.get('desc', 'N/A')}")
                    print()
                
                # ä¿å­˜å›¾ç‰‡ä¿¡æ¯åˆ°æ–‡ä»¶
                images_info = f"å›¾ç‰‡æ€»æ•°: {len(images)}\n\n"
                for i, img in enumerate(images):
                    images_info += f"=== å›¾ç‰‡ {i+1} ===\n"
                    images_info += f"URL: {img.get('src', 'N/A')}\n"
                    images_info += f"Altæ–‡æœ¬: {img.get('alt', 'N/A')}\n"
                    images_info += f"æè¿°: {img.get('desc', 'N/A')}\n\n"
                
                images_path = os.path.join(OUTPUT_PATH, 'images_info.txt')
                with open(images_path, 'w', encoding='utf-8') as f:
                    f.write(images_info)
                print(f"ğŸ’¾ å›¾ç‰‡ä¿¡æ¯å·²ä¿å­˜åˆ°: {images_path}")
                
            else:
                print("âš ï¸ æœªæ‰¾åˆ°å›¾ç‰‡ä¿¡æ¯")
                
        return True
        
    except Exception as e:
        print(f"âŒ å›¾ç‰‡ä¿¡æ¯æå–æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_comprehensive_extraction():
    """ç»¼åˆæµ‹è¯•ï¼šåŒæ—¶æå–å¤šç§ä¿¡æ¯"""
    try:
        print("ğŸ” ç»¼åˆæµ‹è¯•ï¼šåŒæ—¶æå–å¤šç§ä¿¡æ¯...")
        
        from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig
        from crawl4ai.content_filter_strategy import PruningContentFilter
        from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
        
        # æµè§ˆå™¨é…ç½®
        browser_config = BrowserConfig(
            headless=True,
            viewport_width=1280,
            viewport_height=720,
            user_agent='Chrome/114.0.0.0',
        )
        
        # é«˜çº§é…ç½®ï¼šåŒæ—¶å¯ç”¨å¤šç§åŠŸèƒ½
        run_config = CrawlerRunConfig(
            markdown_generator=DefaultMarkdownGenerator(
                content_filter=PruningContentFilter(threshold=0.8),
                options={
                    "ignore_links": False,  # ä¿ç•™é“¾æ¥ä»¥ä¾¿åˆ†æ
                    "ignore_images": False,  # ä¿ç•™å›¾ç‰‡ä»¥ä¾¿åˆ†æ
                }
            ),
            pdf=True,
            screenshot=True,
        )
        
        # åˆ›å»ºçˆ¬è™«å¹¶æ‰§è¡Œ
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(
                url="https://www.anthropic.com/news/agent-capabilities-api",
                config=run_config,
            )
            
            print("ğŸ“Š ç»¼åˆåˆ†æç»“æœ:")
            print(f"  - Markdowné•¿åº¦: {len(result.markdown.fit_markdown if hasattr(result.markdown, 'fit_markdown') else result.markdown)}")
            print(f"  - PDFå¤§å°: {len(result.pdf) if result.pdf else 0} å­—èŠ‚")
            print(f"  - æˆªå›¾å¤§å°: {len(result.screenshot) if result.screenshot else 0} å­—èŠ‚")
            
            # ä¿å­˜ç»¼åˆç»“æœ
            if hasattr(result.markdown, 'fit_markdown'):
                markdown_content = result.markdown.fit_markdown
            else:
                markdown_content = result.markdown
                
            md_path = os.path.join(OUTPUT_PATH, 'markdown', 'comprehensive_test.md')
            with open(md_path, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            print(f"  - Markdownå·²ä¿å­˜åˆ°: {md_path}")
            
            if result.pdf:
                pdf_path = os.path.join(OUTPUT_PATH, 'pdf', 'comprehensive_test.pdf')
                with open(pdf_path, 'wb') as f:
                    f.write(result.pdf)
                print(f"  - PDFå·²ä¿å­˜åˆ°: {pdf_path}")
            
            if result.screenshot:
                screenshot_path = os.path.join(OUTPUT_PATH, 'screenshots', 'comprehensive_test.png')
                with open(screenshot_path, 'wb') as f:
                    f.write(result.screenshot)
                print(f"  - æˆªå›¾å·²ä¿å­˜åˆ°: {screenshot_path}")
            
        return True
        
    except Exception as e:
        print(f"âŒ ç»¼åˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹Crawl4AIæ‰©å±•åŠŸèƒ½æµ‹è¯•...")
    print("=" * 60)
    
    # åˆ›å»ºç›®å½•
    create_directories()
    
    # 1. æµ‹è¯•PDFå’Œæˆªå›¾
    if not await test_pdf_and_screenshot():
        return
        
    print("-" * 60)
    
    # 2. æµ‹è¯•é“¾æ¥æå–
    if not await test_links_extraction():
        return
        
    print("-" * 60)
    
    # 3. æµ‹è¯•å›¾ç‰‡ä¿¡æ¯æå–
    if not await test_images_extraction():
        return
        
    print("-" * 60)
    
    # 4. ç»¼åˆæµ‹è¯•
    if not await test_comprehensive_extraction():
        return
        
    print("=" * 60)
    print("ğŸ‰ æ‰€æœ‰æ‰©å±•åŠŸèƒ½æµ‹è¯•é€šè¿‡!")

if __name__ == "__main__":
    asyncio.run(main()) 