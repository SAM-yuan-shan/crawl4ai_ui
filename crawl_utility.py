"""
Crawl4AI å®ç”¨å·¥å…·è„šæœ¬
æä¾›ç®€å•æ˜“ç”¨çš„æ¥å£æ¥æ‰§è¡Œå¸¸è§çš„çˆ¬å–ä»»åŠ¡
"""

import asyncio
import nest_asyncio
import argparse
import json
import os
from pathlib import Path
from datetime import datetime
import base64

# åº”ç”¨nest_asyncioä»¥æ”¯æŒåœ¨å·²æœ‰äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œ
nest_asyncio.apply()

from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.content_filter_strategy import PruningContentFilter, BM25ContentFilter
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

class CrawlUtility:
    """Crawl4AI å®ç”¨å·¥å…·ç±»"""
    
    def __init__(self, output_dir="outputs"):
        """åˆå§‹åŒ–å·¥å…·"""
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
    async def simple_crawl(self, url, output_file=None):
        """ç®€å•çˆ¬å–ç½‘é¡µå†…å®¹"""
        print(f"ğŸŒ å¼€å§‹çˆ¬å–: {url}")
        
        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun(url=url)
            
            if result.success:
                print(f"âœ… çˆ¬å–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(result.markdown)} å­—ç¬¦")
                
                # ä¿å­˜åˆ°æ–‡ä»¶
                if output_file:
                    output_path = self.output_dir / output_file
                else:
                    filename = url.replace("https://", "").replace("http://", "").replace("/", "_")
                    output_path = self.output_dir / f"{filename}.md"
                
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(result.markdown)
                
                print(f"ğŸ“ å·²ä¿å­˜åˆ°: {output_path}")
                return result.markdown
            else:
                print(f"âŒ çˆ¬å–å¤±è´¥: {result.error_message}")
                return None
                
    async def clean_crawl(self, url, keywords=None, output_file=None):
        """çˆ¬å–å¹¶æ¸…æ´—ç½‘é¡µå†…å®¹"""
        print(f"ğŸ§¹ å¼€å§‹æ¸…æ´—çˆ¬å–: {url}")
        
        # é€‰æ‹©è¿‡æ»¤ç­–ç•¥
        if keywords:
            content_filter = BM25ContentFilter(user_query=keywords)
            print(f"ğŸ” ä½¿ç”¨å…³é”®è¯è¿‡æ»¤: {keywords}")
        else:
            content_filter = PruningContentFilter()
            print("ğŸ”§ ä½¿ç”¨æ™ºèƒ½å†…å®¹ä¿®å‰ª")
        
        run_config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS,
            content_filter=content_filter,
            markdown_generator=DefaultMarkdownGenerator(
                options={"ignore_links": True, "ignore_images": True}
            )
        )
        
        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun(url=url, config=run_config)
            
            if result.success:
                print(f"âœ… æ¸…æ´—å®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(result.markdown)} å­—ç¬¦")
                
                # ä¿å­˜åˆ°æ–‡ä»¶
                if output_file:
                    output_path = self.output_dir / output_file
                else:
                    filename = url.replace("https://", "").replace("http://", "").replace("/", "_")
                    suffix = f"_filtered_{keywords}" if keywords else "_cleaned"
                    output_path = self.output_dir / f"{filename}{suffix}.md"
                
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(result.markdown)
                
                print(f"ğŸ“ å·²ä¿å­˜åˆ°: {output_path}")
                return result.markdown
            else:
                print(f"âŒ æ¸…æ´—å¤±è´¥: {result.error_message}")
                return None
                
    async def pdf_export(self, url, output_file=None):
        """å¯¼å‡ºç½‘é¡µä¸ºPDF"""
        print(f"ğŸ“„ å¼€å§‹PDFå¯¼å‡º: {url}")
        
        run_config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS,
            pdf=True
        )
        
        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun(url=url, config=run_config)
            
            if result.success and result.pdf:
                print(f"âœ… PDFç”ŸæˆæˆåŠŸï¼Œå¤§å°: {len(result.pdf) / 1024:.1f} KB")
                
                # ä¿å­˜PDF
                if output_file:
                    output_path = self.output_dir / output_file
                else:
                    filename = url.replace("https://", "").replace("http://", "").replace("/", "_")
                    output_path = self.output_dir / f"{filename}.pdf"
                
                with open(output_path, 'wb') as f:
                    f.write(result.pdf)
                
                print(f"ğŸ“ å·²ä¿å­˜åˆ°: {output_path}")
                return str(output_path)
            else:
                print(f"âŒ PDFå¯¼å‡ºå¤±è´¥: {result.error_message if not result.success else 'PDFç”Ÿæˆå¤±è´¥'}")
                return None
                
    async def screenshot(self, url, output_file=None):
        """æˆªå–ç½‘é¡µæˆªå›¾"""
        print(f"ğŸ“¸ å¼€å§‹æˆªå›¾: {url}")
        
        run_config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS,
            screenshot=True
        )
        
        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun(url=url, config=run_config)
            
            if result.success and result.screenshot:
                try:
                    # è§£ç base64æˆªå›¾
                    screenshot_data = base64.b64decode(result.screenshot)
                    print(f"âœ… æˆªå›¾æˆåŠŸï¼Œå¤§å°: {len(screenshot_data) / 1024 / 1024:.1f} MB")
                    
                    # ä¿å­˜æˆªå›¾
                    if output_file:
                        output_path = self.output_dir / output_file
                    else:
                        filename = url.replace("https://", "").replace("http://", "").replace("/", "_")
                        output_path = self.output_dir / f"{filename}.png"
                    
                    with open(output_path, 'wb') as f:
                        f.write(screenshot_data)
                    
                    print(f"ğŸ“ å·²ä¿å­˜åˆ°: {output_path}")
                    return str(output_path)
                except Exception as e:
                    print(f"âŒ æˆªå›¾å¤„ç†å¤±è´¥: {str(e)}")
                    return None
            else:
                print(f"âŒ æˆªå›¾å¤±è´¥: {result.error_message if not result.success else 'æˆªå›¾ç”Ÿæˆå¤±è´¥'}")
                return None
                
    async def extract_info(self, url, output_file=None):
        """æå–ç½‘é¡µä¿¡æ¯"""
        print(f"â„¹ï¸ å¼€å§‹ä¿¡æ¯æå–: {url}")
        
        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun(url=url)
            
            if result.success:
                # æå–ä¿¡æ¯
                info = {
                    "url": result.url,
                    "title": getattr(result, 'title', 'N/A'),
                    "content_length": len(result.markdown),
                    "word_count": len(result.markdown.split()),
                    "links": {
                        "internal": len(result.links.get('internal', [])),
                        "external": len(result.links.get('external', []))
                    },
                    "images": len(result.media.get('images', [])) if result.media else 0,
                    "extract_time": datetime.now().isoformat()
                }
                
                print(f"âœ… ä¿¡æ¯æå–å®Œæˆ")
                print(f"   æ ‡é¢˜: {info['title']}")
                print(f"   å†…å®¹é•¿åº¦: {info['content_length']} å­—ç¬¦")
                print(f"   å†…éƒ¨é“¾æ¥: {info['links']['internal']} ä¸ª")
                print(f"   å¤–éƒ¨é“¾æ¥: {info['links']['external']} ä¸ª")
                print(f"   å›¾ç‰‡: {info['images']} ä¸ª")
                
                # ä¿å­˜ä¿¡æ¯
                if output_file:
                    output_path = self.output_dir / output_file
                else:
                    filename = url.replace("https://", "").replace("http://", "").replace("/", "_")
                    output_path = self.output_dir / f"{filename}_info.json"
                
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(info, f, ensure_ascii=False, indent=2)
                
                print(f"ğŸ“ å·²ä¿å­˜åˆ°: {output_path}")
                return info
            else:
                print(f"âŒ ä¿¡æ¯æå–å¤±è´¥: {result.error_message}")
                return None
                
    async def batch_crawl(self, urls, output_dir=None):
        """æ‰¹é‡çˆ¬å–å¤šä¸ªURL"""
        print(f"ğŸ”„ å¼€å§‹æ‰¹é‡çˆ¬å– {len(urls)} ä¸ªURL")
        
        if output_dir:
            batch_output_dir = Path(output_dir)
        else:
            batch_output_dir = self.output_dir / f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        batch_output_dir.mkdir(exist_ok=True)
        
        results = []
        
        async with AsyncWebCrawler() as crawler:
            for i, url in enumerate(urls, 1):
                print(f"  ğŸ“„ [{i}/{len(urls)}] {url}")
                
                try:
                    result = await crawler.arun(url=url)
                    
                    if result.success:
                        # ç”Ÿæˆæ–‡ä»¶å
                        filename = url.replace("https://", "").replace("http://", "").replace("/", "_")
                        output_file = batch_output_dir / f"{i:03d}_{filename}.md"
                        
                        # ä¿å­˜å†…å®¹
                        with open(output_file, 'w', encoding='utf-8') as f:
                            f.write(result.markdown)
                        
                        results.append({
                            "url": url,
                            "success": True,
                            "file": str(output_file),
                            "length": len(result.markdown)
                        })
                        
                        print(f"     âœ… æˆåŠŸï¼Œ{len(result.markdown)} å­—ç¬¦")
                    else:
                        results.append({
                            "url": url,
                            "success": False,
                            "error": result.error_message
                        })
                        print(f"     âŒ å¤±è´¥: {result.error_message}")
                        
                except Exception as e:
                    results.append({
                        "url": url,
                        "success": False,
                        "error": str(e)
                    })
                    print(f"     âŒ å¼‚å¸¸: {str(e)}")
        
        # ä¿å­˜æ‰¹é‡ç»“æœæŠ¥å‘Š
        report_file = batch_output_dir / "batch_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump({
                "total_urls": len(urls),
                "successful": sum(1 for r in results if r["success"]),
                "failed": sum(1 for r in results if not r["success"]),
                "results": results,
                "created_at": datetime.now().isoformat()
            }, f, ensure_ascii=False, indent=2)
        
        successful = sum(1 for r in results if r["success"])
        print(f"ğŸ‰ æ‰¹é‡çˆ¬å–å®Œæˆ: {successful}/{len(urls)} æˆåŠŸ")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {batch_output_dir}")
        
        return results

def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(description="Crawl4AI å®ç”¨å·¥å…·")
    parser.add_argument("command", choices=["simple", "clean", "pdf", "screenshot", "info", "batch"], 
                        help="æ‰§è¡Œçš„å‘½ä»¤")
    parser.add_argument("url", nargs="?", help="ç›®æ ‡URLï¼ˆbatchæ¨¡å¼ä¸‹ä¸ºæ–‡ä»¶è·¯å¾„ï¼‰")
    parser.add_argument("-o", "--output", help="è¾“å‡ºæ–‡ä»¶å")
    parser.add_argument("-k", "--keywords", help="å…³é”®è¯è¿‡æ»¤ï¼ˆä»…cleanæ¨¡å¼ï¼‰")
    parser.add_argument("--output-dir", default="outputs", help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # åˆ›å»ºå·¥å…·å®ä¾‹
    utility = CrawlUtility(args.output_dir)
    
    async def run_command():
        if args.command == "simple":
            if not args.url:
                print("âŒ è¯·æä¾›URL")
                return
            await utility.simple_crawl(args.url, args.output)
            
        elif args.command == "clean":
            if not args.url:
                print("âŒ è¯·æä¾›URL")
                return
            await utility.clean_crawl(args.url, args.keywords, args.output)
            
        elif args.command == "pdf":
            if not args.url:
                print("âŒ è¯·æä¾›URL")
                return
            await utility.pdf_export(args.url, args.output)
            
        elif args.command == "screenshot":
            if not args.url:
                print("âŒ è¯·æä¾›URL")
                return
            await utility.screenshot(args.url, args.output)
            
        elif args.command == "info":
            if not args.url:
                print("âŒ è¯·æä¾›URL")
                return
            await utility.extract_info(args.url, args.output)
            
        elif args.command == "batch":
            if not args.url:
                print("âŒ è¯·æä¾›URLåˆ—è¡¨æ–‡ä»¶è·¯å¾„")
                return
            
            # è¯»å–URLåˆ—è¡¨
            try:
                with open(args.url, 'r', encoding='utf-8') as f:
                    urls = [line.strip() for line in f if line.strip() and not line.startswith('#')]
                await utility.batch_crawl(urls, args.output)
            except FileNotFoundError:
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.url}")
            except Exception as e:
                print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    # è¿è¡Œå‘½ä»¤
    asyncio.run(run_command())

if __name__ == "__main__":
    main() 